{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) with LoRA/QLoRA using TRL ‚Äî on a Free Colab Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/cookbook/blob/main/notebooks/en/fine_tuning_llm_grpo_trl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![trl banner](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_banner_dark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easily fine-tune Large Language Models (LLMs) or Vision-Language Models (VLMs) with **LoRA** or **QLoRA** using the [**Transformers Reinforcement Learning (TRL)**](https://github.com/huggingface/trl) library built by Hugging Face ‚Äî all within a **free Google Colab notebook** (powered by a **T4 GPU**.).  \n",
    "\n",
    "- [TRL GitHub Repository](https://github.com/huggingface/trl) ‚Äî star us to support the project!  \n",
    "- [Official TRL Examples](https://huggingface.co/docs/trl/example_overview)  \n",
    "- [Community Tutorials](https://huggingface.co/docs/trl/community_tutorials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to perform **Supervised Fine-Tuning (SFT)** with **LoRA/QLoRA** using **TRL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "We'll install **TRL** with the **PEFT** extra, which ensures all main dependencies such as **Transformers** and **PEFT** are included.\n",
    "Additionally, we'll install **trackio** to log and monitor our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq \"trl[peft]\" trackio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log in to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log in to your **Hugging Face** account to save your fine-tuned model, track your experiment results directly on the Hub or access gated models. You can find your **access token** on your [account settings page](https://huggingface.co/settings/tokens).\n",
    "\n",
    "You can skip this cell and the next if you don‚Äôt want to save your model, although doing so may lead to unexpected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll retrieve your **Hugging Face username** to use it as part of the path for saving and loading the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "username = api.whoami()[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not logged in to Hugging Face, uncomment and run the cell below to avoid potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "In this step, we load the **Capybara** dataset from the Hugging Face Hub using the `datasets` library.  \n",
    "**Capybara** is a collection of high-quality, multi-turn synthetic conversations generated. It emphasizes reasoning, logic, and information diversity across a wide range of domains‚Äîmaking it ideal for training models capable of engaging in complex and natural dialogues.  \n",
    "\n",
    "Here, we load only the **training split** for efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"trl-lib/Capybara\"\n",
    "train_dataset = load_dataset(dataset_name, split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and configure LoRA/QLoRA\n",
    "\n",
    "There are two ways to use this notebook, depending on your chosen fine-tuning method:\n",
    "\n",
    "- **Using QLoRA:** Run the following cell as is.  \n",
    "- **Using LoRA:** Comment out the installation instruction and update the `use_qlora` parameter accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using QLoRA\n",
    "!pip install -Uq bitsandbytes\n",
    "\n",
    "use_qlora = True\n",
    "# use_qlora = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, choose your **preferred model**. All of the options have been tested on **free Colab instances**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- üîß Model selection ---\n",
    "\n",
    "# Select one model below by uncommenting the line you want to use üëá\n",
    "# (Comments indicate memory usage or warnings)\n",
    "\n",
    "# üß† LoRA models (explicitly LoRA)\n",
    "## Qwen QLoRA\n",
    "model_id, output_dir = \"Qwen/Qwen3-0.6B\", \"Qwen3-0.6B-SFT\"        # ‚úÖ WORKS\n",
    "model_id, output_dir = \"Qwen/Qwen3-1.7B\", \"Qwen3-1.7B-SFT\"        # ‚úÖ WORKS\n",
    "\n",
    "# üß© QLoRA models (default if not LoRA)\n",
    "## Qwen QLoRA\n",
    "# model_id, output_dir = \"Qwen/Qwen2.5-0.5B-Instruct\", \"Qwen2.5-0.5B-Instruct\"\n",
    "# model_id, output_dir = \"Qwen/Qwen3-4B\", \"Qwen3-4B-SFT\"            # ‚úÖ WORKS\n",
    "# model_id, output_dir = \"Qwen/Qwen3-8B\", \"Qwen3-8B-SFT\"            # ‚ö†Ô∏è ~12.8 GB VRAM\n",
    "# model_id, output_dir = \"Qwen/Qwen3-14B\", \"Qwen3-14B-SFT\"          # ‚ùå OOM\n",
    "\n",
    "\n",
    "## LLaMA QLoRA\n",
    "# model_id, output_dir = \"meta-llama/Llama-3.1-8B-Instruct\", \"Llama-3.1-8B-Instruct\"  # ‚ö†Ô∏è ~10.9 GB VRAM\n",
    "# model_id, output_dir = \"meta-llama/Llama-3.2-3B-Instruct\", \"Llama-3.2-3B-Instruct\"  # ‚úÖ ~4.7 GB VRAM\n",
    "\n",
    "## DeepSeek QLoRA\n",
    "# model_id, output_dir = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\", \"DeepSeek-R1-0528-Qwen3-8B\"\n",
    "\n",
    "## Gemma QLoRA\n",
    "# model_id, output_dir = \"google/gemma-3n-E2B-it\", \"gemma-3n-E2B-it\"  # ‚ùå OOM\n",
    "# model_id, output_dir = \"google/gemma-3-4b-it\", \"gemma-3-4b-it\"      # ‚ö†Ô∏è image processing error\n",
    "\n",
    "## Granite QLoRA\n",
    "#model_id, output_dir = \"ibm-granite/granite-4.0-micro\", \"granite-4.0-micro\"  # ‚úÖ ~3.3 GB VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the selected model using `transformers`, configuring QLoRA via `bitsandbytes` if needed. We don't need to configure the tokenizer since the trainer takes care of that automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "kwargs = {\n",
    "    \"attn_implementation\": \"sdpa\",  # Change to Flash Attention if GPU has support\n",
    "    \"dtype\": torch.float16,          # Change to bfloat16 if GPU has support\n",
    "    \"use_cache\": True,\n",
    "    \"max_length\": 2048,\n",
    "}\n",
    "\n",
    "if use_qlora:\n",
    "    kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines LoRA (or QLoRA if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Let's configure SFT using `SFTConfig`. We keep the parameters minimal to fit in a free Colab instance. You can play with them if more resources are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=5,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\" if use_qlora else \"adamw_torch\",\n",
    "    report_to=\"trackio\",\n",
    "    output_dir=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the SFT Trainer. We pass the previously configured `training_args`. We don't use eval dataset to mantain memory usage low but you can configure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show memory stats before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show memory stats after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving fine tuned model\n",
    "\n",
    "In this step, we save the fine-tuned model both **locally** and to the **Hugging Face Hub** using the credentials from your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "trainer.push_to_hub(f\"{username}/{output_dir}\", dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tuned model and run inference\n",
    "\n",
    "Now, let's test the fine-tuned model by loading the **LoRA/QLoRA adapter** and performing **inference**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = model_id\n",
    "adapter_model = f\"{username}/{output_dir}\" # if LoRA/QLoRA\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, dtype=\"auto\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, adapter_model) # if LoRA/QLoRA\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Spain?\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=500\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# Decode and extract model response\n",
    "generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Serving with vLLM\n",
    "\n",
    "You can use Transformer models with **vLLM** to serve them in real-world applications. Learn more [here](https://blog.vllm.ai/2025/04/11/transformers-backend.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Merged Model (for LoRA or QLoRA Training)\n",
    "\n",
    "To serve the model via **vLLM**, the repository must contain the merged model (base model + LoRA adapter). Therefore, you need to upload it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merged = model.merge_and_unload()\n",
    "\n",
    "save_dir = f\"{output_dir}-merged\"\n",
    "\n",
    "model_merged.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merged.push_to_hub(f\"{username}/{output_dir}-merged\")\n",
    "tokenizer.push_to_hub(f\"{username}/{output_dir}-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Inference with vLLM\n",
    "\n",
    "Use **vLLM** to run your model and generate text efficiently in real-time. This allows you to test and deploy your fine-tuned models with low latency and high throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = LLM(model=f\"{username}/{output_dir}-merged\", model_impl=\"transformers\")  # if LoRA/QLoRA\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(f\"{username}/{output_dir}-merged\")  # if LoRA/QLoRA\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Spain?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = hf_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "outputs = llm.generate(\n",
    "    {\"prompt\": prompt},\n",
    "    sampling_params=SamplingParams(max_tokens=500),\n",
    ")\n",
    "\n",
    "for o in outputs:\n",
    "    generated_text = o.outputs[0].text\n",
    "    print(generated_text)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
